{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fa78489",
   "metadata": {},
   "source": [
    "# CSC413 Lab 2: Word Embeddings\n",
    "\n",
    "In this lab, we will build a neural network that can predict the next word\n",
    "in a sentence given the previous three. We will apply an idea called *weight sharing*\n",
    "to go beyond the multi-layer perceptrons that we discussed in class.\n",
    "\n",
    "We will also solve this problem problem twice: once in numpy, and once\n",
    "using PyTorch. When using numpy, you'll implement the backpropagation\n",
    "computation manually.\n",
    "\n",
    "The prediction task is not very interesting on its own, but in learning to predict\n",
    "subsequent words given the previous three, our neural networks will learn\n",
    "about how to *represent* words. In the last part of the lab, we'll explore\n",
    "the *vector representations* of words that our model produces,\n",
    "and analyze these representations.\n",
    "\n",
    "Acknowledgements:\n",
    "\n",
    "- Based on an assignment by George Dahl, Jing Yao Li, Roger Grosse, and Lisa Zhang\n",
    "- Modification by Umangi Jain, Aida Ramezani, and Claas Voelcker\n",
    "\n",
    "## Submission\n",
    "\n",
    "Submit the ipynb file `lab02.ipynb` on Markus.\n",
    "Your notebook file must contain your code, the required equations, **and outputs** where applicable,\n",
    "including printed lines and images.\n",
    "Your TA will not run your code for the purpose of grading.\n",
    "\n",
    "For this lab, you should submit the tasks marked as **Graded Task**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19187928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f6ea2a",
   "metadata": {},
   "source": [
    "## Part 1. Data\n",
    "\n",
    "We will begin by downloading the data onto Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download lab data file\n",
    "!wget https://www.cs.toronto.edu/~lczhang/413/raw_sentences.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff3bbf7",
   "metadata": {},
   "source": [
    "With any machine learning problem, the first thing that we would want to do\n",
    "is to get an intuitive understanding of what our data looks like.\n",
    "The following code reads the sentences in our file, split each sentence into\n",
    "its individual words, and stores the sentences (list of words) in the\n",
    "variable `sentences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0006399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for line in open('raw_sentences.txt'):\n",
    "    words = line.split()\n",
    "    sentence = [word.lower() for word in words]\n",
    "    sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d64bd3",
   "metadata": {},
   "source": [
    "There are 97,162 sentences in total, and \n",
    "these sentences are composed of 250 distinct words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set([w for s in sentences for w in s])\n",
    "print(len(sentences)) # 97162\n",
    "print(len(vocab)) # 250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a71862",
   "metadata": {},
   "source": [
    "We will separate our data into training, validation, and test.\n",
    "We will use 10,000 sentences for test, 10,000 for validation, and\n",
    "the remaining for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aacdc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, randomly shuffle the sentences in case these sentences are\n",
    "# temporally correlated (i.e., so that our train/val/test sets have\n",
    "# equal probability of getting the earlier vs later sentences)\n",
    "import random\n",
    "random.seed(10)\n",
    "random.shuffle(sentences)\n",
    "\n",
    "test, valid, train = sentences[:10000], sentences[10000:20000], sentences[20000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a5653",
   "metadata": {},
   "source": [
    "**Task**: To get an understanding of the data set that we are working with,\n",
    "start by printing 10 sentences in the training set.\n",
    "How are punctuated treated in this word representation? What about\n",
    "words with apostrophes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecfb90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b40652",
   "metadata": {},
   "source": [
    "It is also a good idea to explore the distributional properties of the data.\n",
    "\n",
    "**Task**: The length of the sentences affects the types of modeling we can perform\n",
    "on the data. If the sentences are too short, then using a model that depends on many\n",
    "previous words would not make sense. Run the below code, which computes the length\n",
    "of the shortest, average, and longest sentences in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd80463",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths = [len(s) for s in train]\n",
    "print(\"Shortest Sentence\", np.min(sentence_lengths))\n",
    "print(\"Average Sentence\", np.mean(sentence_lengths))\n",
    "print(\"Longest Sentence\", np.max(sentence_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ac6ec",
   "metadata": {},
   "source": [
    "**Task**: How many words are in the training set? \n",
    "In general, there may be words in the validation/test sets that are *not* in training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cd75d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code to perform the computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43bba26",
   "metadata": {},
   "source": [
    "**Task**: What is the most common word in the training set?\n",
    "How often does this word appear in the training set? \n",
    "This information is useful to know since it helps us understand\n",
    "the difficult of the word prediction problem. In other words, this\n",
    "figure represents the *accuracy* of a \"baseline\" model that\n",
    "simply returns the most common word as the prediction for what the next word should\n",
    "be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aaca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "total_words = 0        # count number of words in the training set\n",
    "word_count = Counter() # count the occurrence of each word\n",
    "for s in train:\n",
    "    for w in s:\n",
    "        word_count[w] += 1\n",
    "        total_words += 1\n",
    "\n",
    "# TODO: find the most common word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223d3539",
   "metadata": {},
   "source": [
    "Now that we understand a bit about the distributional properties of our data,\n",
    "we can move on to representing our data numerically in a way that a neural network\n",
    "can use.\n",
    "\n",
    "We will use a one-hot encoding to represent words. Alternatively,\n",
    "you can think of what we're doing as assigning each word to a unique integer index.\n",
    "We will need some functions that converts sentences into the corresponding\n",
    "word indices.\n",
    "\n",
    "**Graded Task**: Complete the helper functions `convert_words_to_indices`.\n",
    "The functions `generate_4grams` and `process_data` have been written\n",
    "for you. The `process_data` function will take a \n",
    "list of sentences (i.e. list of list of words), and generate an \n",
    "$N \\times 4$ numpy matrix containing indices of 4 words that appear\n",
    "next to each other. You can use the constants `vocab`, `vocab_itos`,\n",
    "and `vocab_stoi` in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5ed989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of all the words in the data set. We will assign a unique \n",
    "# identifier for each of these words.\n",
    "vocab = sorted(list(set([w for s in train for w in s])))\n",
    "# A mapping of index => word (string)\n",
    "vocab_itos = dict(enumerate(vocab))\n",
    "# A mapping of word => its index\n",
    "vocab_stoi = {word:index for index, word in vocab_itos.items()}\n",
    "\n",
    "def convert_words_to_indices(sents):\n",
    "    \"\"\"\n",
    "    This function takes a list of sentences (list of list of words)\n",
    "    and returns a new list with the same structure, but where each word\n",
    "    is replaced by its index in `vocab_stoi`.\n",
    "\n",
    "    Example:\n",
    "    >>> convert_words_to_indices([['one', 'in', 'five', 'are', 'over', 'here'],\n",
    "                                  ['other', 'one', 'since', 'yesterday'],\n",
    "                                  ['you']])\n",
    "    [[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]]\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    # TODO: Write your code here\n",
    "    return indices\n",
    "\n",
    "def generate_4grams(seqs):\n",
    "    \"\"\"\n",
    "    This function takes a list of sentences (list of lists) and returns\n",
    "    a new list containing the 4-grams (four consecutively occurring words)\n",
    "    that appear in the sentences. Note that a unique 4-gram can appear multiple\n",
    "    times, one per each time that the 4-gram appears in the data parameter `seqs`.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> generate_4grams([[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]])\n",
    "    [[148, 98, 70, 23], [98, 70, 23, 154], [70, 23, 154, 89], [151, 148, 181, 246]]\n",
    "    >>> generate_4grams([[1, 1, 1, 1, 1]])\n",
    "    [[1, 1, 1, 1], [1, 1, 1, 1]]\n",
    "    \"\"\"\n",
    "    grams = []\n",
    "    for seq in seqs:\n",
    "        for i in range(len(seq) - 3):\n",
    "            grams.append(seq[i:i+4])\n",
    "    return grams\n",
    "\n",
    "def process_data(sents):\n",
    "    \"\"\"\n",
    "    This function takes a list of sentences (list of lists), and generates an\n",
    "    numpy matrix with shape [N, 4] containing indices of words in 4-grams.\n",
    "    \"\"\"\n",
    "    indices = convert_words_to_indices(sents)\n",
    "    fourgrams = generate_4grams(indices)\n",
    "    return np.array(fourgrams)\n",
    "\n",
    "train4grams = process_data(train)\n",
    "valid4grams = process_data(valid)\n",
    "test4grams = process_data(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa867cf0",
   "metadata": {},
   "source": [
    "**Task**: We are almost ready to discuss the model. Review the following helper\n",
    "functions, which has been written for you:\n",
    "\n",
    "- `make_onehot`, which converts indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9177cc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_onehot(indicies, total=250):\n",
    "    \"\"\"\n",
    "    Convert indicies into one-hot vectors.\n",
    "\n",
    "    Parameters:\n",
    "        `indices` - a numpy array of any shape (e.g. `[N, 3]` where `N`\n",
    "                    is the batch size)\n",
    "        `total` - an integer describing the total number of possible classes\n",
    "                  (maximum possible value in `indicies`)\n",
    "\n",
    "    Returns: a one-hot representation of the input numpy array \n",
    "             (If the input is of shape `[X, Y]`, then the output would\n",
    "             be of shape `[X, Y, total]` and consists of 0's and 1's)\n",
    "    \"\"\"\n",
    "    # create an identity matrix of shape [total, total]\n",
    "    I = np.eye(total)\n",
    "    # index the appropriate columns of that identity matrix\n",
    "    return I[indicies]\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax of vector x, or row-wise for a matrix x.\n",
    "    We subtract x.max(axis=0) from each row for numerical stability.\n",
    "\n",
    "    Parameters:\n",
    "        `x` - a numpy array shape `[N, num_classes]`\n",
    "\n",
    "    Returns: a numpy array of the same shape as the input.\n",
    "    \"\"\"\n",
    "    x = x.T\n",
    "    exps = np.exp(x - x.max(axis=0))\n",
    "    probs = exps / np.sum(exps, axis=0)\n",
    "    return probs.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666c816a",
   "metadata": {},
   "source": [
    "There is one more data processing function that we need,\n",
    "which turns the four-grams into inputs (consisting of the \n",
    "one-hot representations of the first 3 words),\n",
    "and the target output (the index of the 4th word). \n",
    "\n",
    "Since the one-hot representation is not memory efficient, we will\n",
    "only convert data into this representation when required, and only\n",
    "do so at a minibatch level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a85f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, range_min, range_max, onehot=True):\n",
    "    \"\"\"\n",
    "    Convert one batch of data (specifically, `data[range_min:range_max]`)\n",
    "    in the form of 4-grams into input and output data and return the\n",
    "    training data.\n",
    "\n",
    "    Parameters:\n",
    "        `data` - a numpy array of shape [N, 4] produced by a call\n",
    "                 to the function `process_data`\n",
    "        `range_min` - the starting index of the minibatch\n",
    "        `range_max` - the ending index of the minibatch, with\n",
    "                      range_max > range_min and\n",
    "                      batch_size = range_max - range_min\n",
    "        `onehot` - boolean value, if `True` the targets are also made\n",
    "                   to be one-hot vectors rather than indices\n",
    "\n",
    "    Returns: a tuple `(x, t)` where\n",
    "     - `x` is an numpy array of one-hot vectors of shape [batch_size, 3, 250]\n",
    "     - `t` is either\n",
    "            - a numpy array of shape [batch_size, 250] if onehot is True,\n",
    "            - a numpy array of shape [batch_size] containing indicies otherwise\n",
    "    \"\"\"\n",
    "    x = data[range_min:range_max, :3]\n",
    "    x = make_onehot(x)\n",
    "    x = x.reshape(-1, 750)\n",
    "    t = data[range_min:range_max, 3]\n",
    "    if onehot:\n",
    "        t = make_onehot(t).reshape(-1, 250)\n",
    "    return x, t\n",
    "\n",
    "# some testing code for illustrative purposes\n",
    "x_, t_ = get_batch(train4grams, 0, 10, onehot=False)\n",
    "print(train4grams[0])\n",
    "pos_index = train4grams[0][0]\n",
    "print(x_[0, pos_index-1]) # should be 0\n",
    "print(x_[0, pos_index])   # should be 1\n",
    "print(x_[0, pos_index+1]) # should be 0\n",
    "pos_index = train4grams[1][0]\n",
    "print(x_[0, 250 + pos_index-1]) # should be 0\n",
    "print(x_[0, 250 + pos_index])   # should be 1\n",
    "print(x_[0, 250 + pos_index+1]) # should be 0\n",
    "pos_index = train4grams[2][0]\n",
    "print(x_[0, 500 + pos_index-1]) # should be 0\n",
    "print(x_[0, 500 + pos_index])   # should be 1\n",
    "print(x_[0, 500 + pos_index+1]) # should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1992e952",
   "metadata": {},
   "source": [
    "Finally, the `estimate_accuracy` function has been provided to you\n",
    "as well. This function is similar to the `accuracy` function from\n",
    "lab 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e443b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_accuracy(model, data, batch_size=5000, max_N=10000):\n",
    "    \"\"\"\n",
    "    Estimate the accuracy of the model on the data. To reduce\n",
    "    computation time, use at least `max_N` elements of `data` to\n",
    "    produce the estimate.\n",
    "\n",
    "    Parameters:\n",
    "        `model` - an object (e.g. `NNModel`, see below) with a forward()\n",
    "                  method that produces predictions for an input\n",
    "        `data` - a dataset of 4grams (produced by `process_data`) over\n",
    "                 which we compute accuracy\n",
    "        `batch_size` - integer batch size to use to produce predictions\n",
    "        `max_N` - integer value describing the minimum number of predictions\n",
    "                  to make to produce the accuracy estimate\n",
    "\n",
    "    Returns: a floating point value between 0 and 1\n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    num_preds = 0\n",
    "    for i in range(0, data.shape[0], batch_size):\n",
    "        xs, ts = get_batch(data, i, i + batch_size, onehot=False)\n",
    "        z = model.forward(xs)\n",
    "        pred = np.argmax(z, axis=1)\n",
    "        num_correct += np.sum(ts == pred)\n",
    "        num_preds += ts.shape[0]\n",
    "\n",
    "        if num_preds >= max_N: # at least max_N predictions have been made\n",
    "            break\n",
    "    return num_correct / num_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c949f91",
   "metadata": {},
   "source": [
    "## Part 2. Model Building: Forward Pass\n",
    "\n",
    "In this section, we will build our deep learning model.\n",
    "As we did in the previous lab, we begin by understanding how to make\n",
    "predictions with this model.\n",
    "So, in this part of the lab, we will write the functions \n",
    "required to perform the forward pass  operation.\n",
    "We will write the backward-pass and train the model in Part 3 and 4. \n",
    "\n",
    "**Task**: Consider the following two models architectures:\n",
    "\n",
    "Model 1:\n",
    "<img src=\"https://www.cs.toronto.edu/~lczhang/321/hw/p2_model1.png\" />\n",
    "\n",
    "Model 2:\n",
    "<img src=\"https://www.cs.toronto.edu/~lczhang/321/hw/p2_model2.png\" />\n",
    "\n",
    "In Model 1,  the input $\\bf{x}$ consists of three one-hot vectors concatenated\n",
    "together. We can think of $\\bf{h}$ as a representation of those three words\n",
    "(all together). \n",
    "However, the model architecture treat the three one-hot vectors\n",
    "from the three words distinctly.\n",
    "However, $\\bf{W^{(1)}}$ needs to learn about the first word\n",
    "separately from the second and third word. In other words,\n",
    "the deep learning model treats these three sets of\n",
    "one-hot features as if they have no semantic connection in common.\n",
    "\n",
    "In *Model 2*, we use an idea called *weight sharing*, where we use the\n",
    "sample set of weights $\\bf{W}^{(word)}$ to map the one-hot vectors into\n",
    "a vector representation. This allows us to learn the weights $\\bf{W}^{(word)}$\n",
    "from information from all three words. This model architecture encodes our\n",
    "knowledge that the three sets of one-hot vectors share something in common.\n",
    "\n",
    "We will use model 2 in the rest of this lab. For clarity, here is the \n",
    "forward-pass computation to be performed. (Note that this is *not* vectorized!)\n",
    "\n",
    "\\begin{align*}\n",
    "\\bf{x_a} &= \\textrm{the one-hot vector for word 1} \\\\\n",
    "\\bf{x_b} &= \\textrm{the one-hot vector for word 2} \\\\\n",
    "\\bf{x_c} &= \\textrm{the one-hot vector for word 3} \\\\\n",
    "\\bf{v_a} &= \\bf{W}^{(word)} \\bf{x_a} \\\\\n",
    "\\bf{v_b} &= \\bf{W}^{(word)} \\bf{x_b} \\\\\n",
    "\\bf{v_c} &= \\bf{W}^{(word)} \\bf{x_c} \\\\\n",
    "\\bf{v} &= \\textrm{concatenate}(\\bf{v_a}, \\bf{v_b}, \\bf{v_c})\\\\\n",
    "\\bf{m} &= \\bf{W^{(1)}} \\bf{v} + \\bf{b^{(1)}} \\\\\n",
    "\\bf{h} &= \\textrm{ReLU}(\\bf{m}) \\\\\n",
    "\\bf{z} &= \\bf{W^{(2)}} \\bf{h} + \\bf{b^{(2)}} \\\\\n",
    "\\bf{y} &= \\textrm{softmax}(\\bf{z}) \\\\\n",
    "L &= \\mathcal{L}_\\textrm{Cross-Entropy}(\\bf{y}, \\bf{t}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The class `NNModel` represents this above neural network model.\n",
    "This class stores the weights and biases of our model.\n",
    "Moreover, this class will also have methods that use and modify these weights.\n",
    "\n",
    "Most of the class has been implemented for you, including these methods:\n",
    "\n",
    "- The `initializeParams()` method, which randomly initializes the weights\n",
    "- The `loss()` method, which computes the average cross-entropy loss\n",
    "- The `update()` method, which performs the gradient updates\n",
    "- The `cleanup()` method, which clears the member variables used in the computation\n",
    "\n",
    "The implementation for these methods are incomplete:\n",
    "\n",
    "- The `forward` method computes the prediction given a data matrix `X`.\n",
    "  These computations are known as the **forward pass**.\n",
    "  This method also saves some of the intermediate values in the neural network\n",
    "  computation, to make gradient computation easier.\n",
    "- The `backward` method computes the gradient of the average loss\n",
    "  with respect to various quantities (i.e. the error signals).\n",
    "  These computations are known as the **backward pass**.\n",
    "\n",
    "You may assume that during an iteration of gradient descent, the following methods\n",
    "will be called in order:\n",
    "\n",
    "- The `cleanup` method to clear information stored from the previous computation\n",
    "- The `forward` method to compute the predictions\n",
    "- The `backward` method to compute the error signals\n",
    "- (Possibly the `loss` method to compute the average loss)\n",
    "- The `update` method to move the weights\n",
    "\n",
    "You might recognize that the way we set up the class correspond to what PyTorch does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd18ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(object):\n",
    "    def __init__(self, vocab_size=250, emb_size=150, num_hidden=100):\n",
    "        \"\"\"\n",
    "        Initialize the weights and biases of this two-layer MLP.\n",
    "        \"\"\"\n",
    "        # information about the model architecture\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "        # weights for the embedding layer of the model\n",
    "        self.Ww = np.zeros([vocab_size, emb_size])\n",
    "\n",
    "        # weights and biases for the first layer of the MLP\n",
    "        self.W1 = np.zeros([emb_size * 3, num_hidden])\n",
    "        self.b1 = np.zeros([num_hidden])\n",
    "\n",
    "        # weights and biases for the second layer of the MLP\n",
    "        self.W2 = np.zeros([num_hidden, vocab_size])\n",
    "        self.b2 = np.zeros([vocab_size])\n",
    "\n",
    "        # initialize the weights and biases\n",
    "        self.initializeParams()\n",
    "\n",
    "        # set all values of intermediate variables (to be used in the\n",
    "        # forward/backward passes) to None\n",
    "        self.cleanup()\n",
    "\n",
    "    def initializeParams(self):\n",
    "        \"\"\"\n",
    "        Initialize the weights and biases of this two-layer MLP to be random.\n",
    "        This random initialization is necessary to break the symmetry in the\n",
    "        gradient descent update for our hidden weights and biases. If all our\n",
    "        weights were initialized to the same value, then their gradients will\n",
    "        all be the same!\n",
    "        \"\"\"\n",
    "        self.Ww = np.random.normal(0, 2/self.vocab_size, self.Ww.shape)\n",
    "        self.W1 = np.random.normal(0, 2/(3*self.emb_size), self.W1.shape)\n",
    "        self.b1 = np.random.normal(0, 2/(3*self.emb_size), self.b1.shape)\n",
    "        self.W2 = np.random.normal(0, 2/self.num_hidden, self.W2.shape)\n",
    "        self.b2 = np.random.normal(0, 2/self.num_hidden, self.b2.shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Compute the forward pass to produce prediction logits.\n",
    "\n",
    "        Parameters:\n",
    "            `X` - A numpy array of shape (N, self.vocab_size * 3)\n",
    "\n",
    "        Returns: A numpy array of logit predictions of shape\n",
    "                 (N, self.vocab_size)\n",
    "        \"\"\"\n",
    "        return do_forward_pass(self, X) # To be implemented below\n",
    "\n",
    "    def backward(self, ts):\n",
    "        \"\"\"\n",
    "        Compute the backward pass, given the ground-truth, one-hot targets.\n",
    "\n",
    "        You may assume that the `forward()` method has been called for the\n",
    "        corresponding input `X`, so that the quantities computed in the\n",
    "        `forward()` method is accessible.\n",
    "\n",
    "        Parameters:\n",
    "            `ts` - A numpy array of shape (N, self.vocab_size)\n",
    "        \"\"\"\n",
    "        return do_backward_pass(self, ts)\n",
    "\n",
    "    def loss(self, ts):\n",
    "        \"\"\"\n",
    "        Compute the average cross-entropy loss, given the ground-truth, one-hot targets.\n",
    "\n",
    "        You may assume that the `forward()` method has been called for the\n",
    "        corresponding input `X`, so that the quantities computed in the\n",
    "        `forward()` method is accessible.\n",
    "\n",
    "        Parameters:\n",
    "            `ts` - A numpy array of shape (N, self.num_classes)\n",
    "        \"\"\"\n",
    "        return np.sum(-ts * np.log(self.y)) / ts.shape[0]\n",
    "\n",
    "    def update(self, alpha):\n",
    "        \"\"\"\n",
    "        Compute the gradient descent update for the parameters of this model.\n",
    "\n",
    "        Parameters:\n",
    "            `alpha` - A number representing the learning rate\n",
    "        \"\"\"\n",
    "        self.Ww = self.Ww - alpha * self.Ww_bar\n",
    "        self.W1 = self.W1 - alpha * self.W1_bar\n",
    "        self.b1 = self.b1 - alpha * self.b1_bar\n",
    "        self.W2 = self.W2 - alpha * self.W2_bar\n",
    "        self.b2 = self.b2 - alpha * self.b2_bar\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"\n",
    "        Erase the values of the variables that we use in our computation.\n",
    "        \"\"\"\n",
    "        # To be filled in during the forward pass\n",
    "        self.N = None # Number of data points in the batch\n",
    "        self.xa = None # word (a)'s one-hot encoding\n",
    "        self.xb = None # word (b)'s one-hot encoding\n",
    "        self.xc = None # word (c)'s one-hot encoding\n",
    "        self.va = None # word (a)'s embedding\n",
    "        self.vb = None # word (b)'s embedding\n",
    "        self.vc = None # word (c)'s embedding\n",
    "        self.v = None  # concatenated embedding\n",
    "        self.m = None  # pre-activation hidden state\n",
    "        self.h = None  # post-activation hidden state\n",
    "        self.z = None  # prediction logit\n",
    "        self.y = None  # prediction softmax\n",
    "\n",
    "        # To be filled in during the backward pass\n",
    "        self.z_bar  = None # The error signal for self.z\n",
    "        self.W2_bar = None # The error signal for self.W2\n",
    "        self.b2_bar = None # The error signal for self.b2\n",
    "        self.h_bar  = None # The error signal for self.h\n",
    "        self.m_bar  = None # The error signal for self.z1\n",
    "        self.W1_bar = None # The error signal for self.W1\n",
    "        self.b1_bar = None # The error signal for self.b1\n",
    "        self.v_bar  = None # The error signal for self.v\n",
    "        self.va_bar = None # The error signal for self.va\n",
    "        self.vb_bar = None # The error signal for self.vb\n",
    "        self.vc_bar = None # The error signal for self.vc\n",
    "        self.Ww_bar = None # The error signal for self.Ww"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6964a",
   "metadata": {},
   "source": [
    "**Graded Task**: Complete the implementation of the `do_forward_pass` method,\n",
    "which computes the predictions given a `NNModel` and a batch of input data.\n",
    "\n",
    "We recommend that you reason about your approach on paper before writing any numpy code.\n",
    "Track the shapes of your quantities carefully! When you finally write your \n",
    "numpy code, print out the shapes of your quantities as you go along, and\n",
    "reason about whether these shapes match your initial expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f73c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_forward_pass(model, X):\n",
    "    \"\"\"\n",
    "    Compute the forward pass to produce prediction logits.\n",
    "\n",
    "    This function also keeps some of the intermediate values in\n",
    "    the neural network computation, to make computing gradients easier.\n",
    "\n",
    "    For the ReLU activation, you may find the function `np.maximum` helpful\n",
    "\n",
    "    Parameters:\n",
    "        `model` - An instance of the class NNModel\n",
    "        `X` - A numpy array of shape (N, model.vocab_size)\n",
    "\n",
    "    Returns: A numpy array of logit predictions of shape\n",
    "             (N, model.vocab_size)\n",
    "    \"\"\"\n",
    "    # populate the input attributes necessary for the\n",
    "    # backward pass\n",
    "    model.N = X.shape[0]\n",
    "    model.X = X\n",
    "\n",
    "    # for xa, xb, xc, we index the appropriate range of X\n",
    "    # (recall that the tensor X has shape [batch_size, 3*vocab_size])\n",
    "    model.xa = X[:, :model.vocab_size]\n",
    "    model.xb = X[:, model.vocab_size:model.vocab_size*2]\n",
    "    model.xc = X[:, model.vocab_size*2:]\n",
    "\n",
    "    # compute the embeddings\n",
    "    model.va = None # TODO\n",
    "    model.vb = None # TODO\n",
    "    model.vc = None # TODO\n",
    "    model.v = np.concatenate([model.va, model.vb, model.vc], axis=1)\n",
    "\n",
    "    # compute the remaining part of the forward pass\n",
    "    model.m = None # TODO - the hidden state value (pre-activation)\n",
    "    model.h = None # TODO - the hidden state value (post ReLU activation)\n",
    "    model.z = None # TODO - the logit scores (pre-activation)\n",
    "    model.y = None # TODO - the class probabilities (post-activation)\n",
    "    return model.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653a83b3",
   "metadata": {},
   "source": [
    "**Task**: One way important way to check your implementation is to run the\n",
    "`forward()` method to ensure that the shapes of your quantities are correct.\n",
    "Run the below code. If you run into shape mismatch issues, print out the\n",
    "shapes of the quantities that you are working with (e.g. `print(model.va.shape)`)\n",
    "and ensure that these shapes are what you expect them to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a363abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of data that we will use for gradient checking\n",
    "# we will use a small batch size of 8. This number is chosen\n",
    "# because it is small, but also because this shape does not\n",
    "# appear elsewhere in our architecture (e.g. vocab size, num hidden)\n",
    "# so that shape mismatch issues are easier to identify.\n",
    "x_, t_ = get_batch(train4grams, 0, 8)\n",
    "model = NNModel()\n",
    "y = model.forward(x_)\n",
    "\n",
    "# TODO: Check that these shapes are correct. What should these shapes be?\n",
    "print(model.va.shape, model.vb.shape, model.vc.shape)\n",
    "print(model.v.shape)\n",
    "print(model.m.shape, model.h.shape)\n",
    "print(model.z.shape, model.y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946df732",
   "metadata": {},
   "source": [
    "At this point, we can work with a pre-trained model by loading\n",
    "weights that are provided to you via the link below. \n",
    "If you would like, you can jump to part 4 first and explore the\n",
    "interesting properties of this model before tackling backpropagation\n",
    "and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3497c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.cs.toronto.edu/~lczhang/413/sentence_pretrained.pk\n",
    "\n",
    "\n",
    "def load_pretrained(model):\n",
    "\timport pickle\n",
    "    assert(model.vocab_size == 250)\n",
    "    assert(model.emb_size   == 150)\n",
    "    assert(model.num_hidden == 100)\n",
    "    Ww, W1, b1, W2, b2 = pickle.load(open(\"sentence_pretrained.pk\", \"rb\"))\n",
    "    model.Ww = Ww\n",
    "    model.W1 = W1\n",
    "    model.b1 = b1\n",
    "    model.W2 = W2\n",
    "    model.b2 = b2\n",
    "    model.cleanup()\n",
    "    return model\n",
    "\n",
    "model = load_pretrained(NNModel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b167a6",
   "metadata": {},
   "source": [
    "## Part 3. Model Building: Backwards Pass\n",
    "\n",
    "We are ready to complete the function that computes the backward pass of\n",
    "our model!\n",
    "\n",
    "You should start by reviewing the lecture slides on backpropagation.\n",
    "One difference between the slides and our implementation here is that the \n",
    "slides express the required computations for computing the gradients of\n",
    "the loss for a *single data point*.\n",
    "However, our implementation of backpropagation is further vectorized to\n",
    "compute gradients of the loss for a *batch consisting of multiple data points*.\n",
    "\n",
    "We begin with applying the backpropagation algorithm on our forward pass\n",
    "steps from earlier. Recall that our model's forward pass is as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\bf{x_a} &= \\textrm{the one-hot vector for word 1} \\\\\n",
    "\\bf{x_b} &= \\textrm{the one-hot vector for word 2} \\\\\n",
    "\\bf{x_c} &= \\textrm{the one-hot vector for word 3} \\\\\n",
    "\\bf{v_a} &= \\bf{W}^{(word)} \\bf{x_a} \\\\\n",
    "\\bf{v_b} &= \\bf{W}^{(word)} \\bf{x_b} \\\\\n",
    "\\bf{v_c} &= \\bf{W}^{(word)} \\bf{x_c} \\\\\n",
    "\\bf{v} &= \\textrm{concatenate}(\\bf{v_a}, \\bf{v_b}, \\bf{v_c})\\\\\n",
    "\\bf{m} &= \\bf{W^{(1)}} \\bf{v} + \\bf{b^{(1)}} \\\\\n",
    "\\bf{h} &= \\textrm{ReLU}(\\bf{m}) \\\\\n",
    "\\bf{z} &= \\bf{W^{(2)}} \\bf{h} + \\bf{b^{(2)}} \\\\\n",
    "\\bf{y} &= \\textrm{softmax}(\\bf{z}) \\\\\n",
    "L &= \\mathcal{L}_\\textrm{Cross-Entropy}(\\bf{y}, \\bf{t}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Following the steps discussed in this week's lecture, derive the\n",
    "backwards steps (Hint: your math homework 01 could help a lot in doing\n",
    "this).\n",
    "\\begin{align*}\n",
    "\\overline{{\\bf z}}  &= \\dots\\\\\n",
    "\\overline{W^{(2)}}  &= \\dots \\\\\n",
    "\\overline{{\\bf b^{(2)}}}  &= \\dots \\\\\n",
    "\\overline{{\\bf h}}  &= \\dots \\\\\n",
    "\\overline{{\\bf z}} &= \\dots \\\\\n",
    "\\overline{W^{(1)}} &= \\dots \\\\\n",
    "\\overline{{\\bf b}^{(1)}} &= \\dots \\\\\n",
    "\\overline{{\\bf m}}  &= \\dots \\\\\n",
    "\\overline{{\\bf v}} &= \\dots \\\\\n",
    "\\overline{{\\bf v_a}} &= \\dots \\\\\n",
    "\\overline{{\\bf v_b}} &= \\dots \\\\\n",
    "\\overline{{\\bf v_c}} &= \\dots \\\\\n",
    "\\overline{{\\bf W^{(word)}}} &= \\dots \\\\\n",
    "\\end{align*}\n",
    "\n",
    "**Graded Task**: What is the error signal $\\overline{{\\bf v_a}}$?\n",
    "How does this quantity relate to $\\overline{{\\bf v}}$? \n",
    "To answer this question, reason about the scalars that make up the elements of\n",
    "$\\overline{{\\bf v}}$. Which of these scalars also appear in  $\\overline{{\\bf v_a}}$?\n",
    "\n",
    "Express your answer by computing `va_bar` (representing the quantity  $\\overline{{\\bf v_a}}$)\n",
    "given `v_bar` (representing the quantity  $\\overline{{\\bf v}}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee87a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "emb_size = 100\n",
    "v_bar = np.random.rand(N, emb_size * 3)\n",
    "\n",
    "va_bar = None # TODO\n",
    "vb_bar = None # TODO\n",
    "vc_bar = None # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575efc1f",
   "metadata": {},
   "source": [
    "**Graded Task**: What is the derivative $\\overline{{\\bf W^{(word)}}}$? \n",
    "You may find it helpful to draw a computation graph,\n",
    "and then remember the multivariate chain rule. If $\\overline{{\\bf W^{(word)}}}$ affects the\n",
    "loss in 3 different paths, what do we do with those 3 gradients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc9fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Work out the derivative on paper and give the final equation here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f978599",
   "metadata": {},
   "source": [
    "We are still not done:\n",
    "the gradient computation is for a single input ${\\bf x}$.\n",
    "We will need to vectorize each of these computations so that they work \n",
    "for an entire batch of inputs ${\\bf X}$ of shape $N \\times 3 \\textrm{vocab_size}$.\n",
    "\n",
    "For some quantities, vectorizing the backward-pass computation is just as \n",
    "straightforward as the forward-pass computation, requiring the same\n",
    "techniques. For example, each \n",
    "input ${\\bf x}$ in a batch will have its own corresponding value of\n",
    "${\\bf z}$ and thus $\\overline{{\\bf z}}. (If this sentence is confusing,\n",
    "check that your description of the shape for `z_bar` from Part 2 has the\n",
    "batch size `N` in there somewhere.)\n",
    "\n",
    "For other quantities, vectorizing requires the use of the multivariate chain rule.\n",
    "For example, there is a single weight matrix $W^{(2)}$, used for all\n",
    "inputs in a batch. Thus, a change in $W^{(2)}$ will affect the predictions for\n",
    "*all* inputs. (If this sentence is confusing,\n",
    "check that your description of the shape for `W2_bar` from Part 2 \n",
    "**does not** have batch size `N` in there.)\n",
    "\n",
    "The vectorization for the quantities consistent with those of a MLP is\n",
    "already provided to you in the `do_backward_pass` function.\n",
    "However, the rest of this function is incomplete.\n",
    "\n",
    "**Graded Task**: Complete the implementation of the `do_backward_pass` function,\n",
    "which performs backpropagation given a `NNModel`, given the ground-truth\n",
    "one-hot targets `ts`. This function assumes that the forward pass method had been \n",
    "called on the input `X` corresponding to those one-hot targets.\n",
    "\n",
    "Once again, we recommend that you reason about your approach on paper before\n",
    "writing any numpy code! In particular, understand the vectorization strategies\n",
    "discussed in the previous weeks and above before proceeding.\n",
    "Track the shapes of your quantities carefully! When you finally write your \n",
    "numpy code, print out the shapes of your quantities as you go along, and\n",
    "reason about whether these shapes match your initial expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a867865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_backward_pass(model, ts):\n",
    "    \"\"\"\n",
    "    Compute the backward pass, given the ground-truth, one-hot targets.\n",
    "\n",
    "    You may assume that `model.forward()` has been called for the\n",
    "    corresponding input `X`, so that the quantities computed in the\n",
    "    `forward()` method is accessible.\n",
    "\n",
    "    The member variables you store here will be used in the `update()`\n",
    "    method. Check that the shapes match what you wrote in Part 2.\n",
    "\n",
    "    Parameters:\n",
    "        `model` - An instance of the class NNModel\n",
    "        `ts` - A numpy array of shape (N, model.num_classes)\n",
    "    \"\"\"\n",
    "    # The gradient signal for the MLP part of this is given\n",
    "    # to you (or worked out together from above, TODO)\n",
    "    model.z_bar = None  # TODO\n",
    "    model.W2_bar = None # TODO\n",
    "    model.b2_bar = None # TODO\n",
    "    model.h_bar = None  # TODO\n",
    "    model.m_bar = None  # TODO\n",
    "    model.W1_bar = None # TODO\n",
    "    model.b1_bar = None # TODO\n",
    "    model.v_bar = None  # TODO\n",
    "\n",
    "    # Refer to your answer above\n",
    "    model.va_bar = None # TODO\n",
    "    model.vb_bar = None # TODO\n",
    "    model.vc_bar = None # TODO\n",
    "\n",
    "    # Refer to your answer above\n",
    "    model.Ww_bar = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e874e236",
   "metadata": {},
   "source": [
    "As we saw in CSC311, debugging machine learning code can be extremely\n",
    "challenging.\n",
    "It helps to **be systematic about testing**, and to test every helper\n",
    "function as we write it.\n",
    "It is important to test `do_backward_pass` before using it for training,\n",
    "so that we can isolates issues related to computing gradients vs. other\n",
    "training issues (e.g. those related to poor hyperparameter choices).\n",
    "\n",
    "**Task**: As in the forward pass,\n",
    "start by making sure that the shapes match.\n",
    "Again, If you run into shape mismatch issues, print out the\n",
    "shapes of the quantities that you are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36545bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_, t_ = get_batch(train4grams, 0, 8)\n",
    "model = NNModel()\n",
    "\n",
    "model.forward(x_)\n",
    "model.backward(t_)\n",
    "model.update(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9008875",
   "metadata": {},
   "source": [
    "The above step checks that the shapes match.\n",
    "But we also saw, in CSC311, that one way to check the gradient\n",
    "computation is through **finite difference**.\n",
    "Recall the definition of a derivative.\n",
    "For a function $g(w): \\mathbb{R} \\rightarrow \\mathbb{R}$,\n",
    "\n",
    "$$g'(w) = \\lim_{h \\rightarrow 0} \\frac{g(w+h) - g(w)}{h}$$\n",
    "\n",
    "This above rule tells us that if we have a way to evaluate `g` and would like to\n",
    "test our implementation of $g'$, we can choose an $h$ small enough, and check if:\n",
    "\n",
    "$$g'(w) \\approx \\frac{g(w+h) - g(w)}{h}$$\n",
    "\n",
    "In our case, we have that for any parameter $w_j$ and an $h$ small enough,\n",
    "we should have for our loss $$\\mathcal{E}$$:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{E}}{\\partial w_j} \\approx \\frac{\\mathcal{E}(w_0, w_1, \\dots, w_{j-1}, w_j + h, w_{j+1}, \\dots, w_D) - \\mathcal{E}(w_0, w_1, \\dots, w_D)}{h}$$\n",
    "\n",
    "\n",
    "(A word about notation: here we are enumerating over all scalar weights \n",
    "$w_0 \\dots w_D$ in our model. You will often see this in machine learning \n",
    "textbooks and papers, where we ignore the fact that these scalar weights come\n",
    "from several different weight matrices and bias vectors.\n",
    "This notation might feel strange/imprecise as first, but keep in mind that\n",
    "mathematical notations is a form of language whose purpose is to communicate\n",
    "ideas. Practitioners choose different notations, and even introduce new notation,\n",
    "with the goal of clearly communicating a specific idea. Here, the idea is that\n",
    "we should be able to test the gradient computation or a single scalar weight by\n",
    "computing the loss function twice: once with a slight perturbation on that\n",
    "scalar weight.)\n",
    "\n",
    "**Graded Task:** Run the below code to spot test that the gradients `Ww_bar` is\n",
    "computed correctly. Include the output of the code in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9aa3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will opt to use a large batch size to test the gradients `Ww_bar`\n",
    "# with a large batch size. Why do you think this is? (Why might we\n",
    "# be more likely to have gradients of value 0 if the batch size is\n",
    "# small?)\n",
    "x_, t_ = get_batch(train4grams, 0, 800)\n",
    "\n",
    "model = NNModel()\n",
    "model.forward(x_)\n",
    "\n",
    "# Check the gradient for Ww_bar[3, 10]. \n",
    "# You should spot check other indices too!\n",
    "model.backward(t_)\n",
    "gradient = model.Ww_bar[3, 10]\n",
    "\n",
    "# we should have \n",
    "# gradient ~= (loss_perturbed - loss_initial) / h\n",
    "# where loss_perturbed is the loss if we perturb \n",
    "# model.Ww_bar[3, 10] by a small value h\n",
    "\n",
    "loss_initial = model.loss(t_)\n",
    "\n",
    "h = 0.01\n",
    "model.Ww[3, 10] += h \n",
    "\n",
    "model.cleanup()\n",
    "model.forward(x_)\n",
    "loss_perturbed = model.loss(t_)\n",
    "\n",
    "# These two values should be close\n",
    "print(gradient)\n",
    "print((loss_perturbed - loss_initial) / h) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c96cc5e",
   "metadata": {},
   "source": [
    "If gradient checking succeeds, we are ready to train our model.\n",
    "The function `train_model` is written for you. Run the code below\n",
    "with the default hyperparameters.\n",
    "Although hyperparameter tuning is an important step in machine learning,\n",
    "we have chosen reasonable hyperparameters to you to keep this lab \n",
    "a reasonable size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7458f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "                train_data=train4grams,\n",
    "                validation_data=valid4grams,\n",
    "                batch_size=50,\n",
    "                learning_rate=0.3,\n",
    "                max_iters=20000,\n",
    "                plot_every=1000):\n",
    "    \"\"\"\n",
    "    Use gradient descent to train the numpy model on the dataset train4grams.\n",
    "    \"\"\"\n",
    "    iters, train_loss, train_acc, val_acc = [], [], [], [] # for the training curve\n",
    "    iter_count = 0  # count the number of iterations\n",
    "    try:\n",
    "        while iter_count < max_iters:\n",
    "            # shuffle the training data, and break early if we don't have\n",
    "            # enough data to remaining in the batch\n",
    "            np.random.shuffle(train_data)\n",
    "            for i in range(0, train_data.shape[0], batch_size):\n",
    "                if (i + batch_size) > train_data.shape[0]:\n",
    "                    break\n",
    "\n",
    "                # get the input and targets of a minibatch\n",
    "                xs, ts = get_batch(train_data, i, i + batch_size, onehot=True)\n",
    "\n",
    "                # erase any accumulated gradients\n",
    "                model.cleanup()\n",
    "\n",
    "                # forward pass: compute prediction\n",
    "                ys = model.forward(xs)\n",
    "\n",
    "                # backward pass: compute error \n",
    "                model.backward(ts)\n",
    "                model.update(learning_rate)\n",
    "\n",
    "                # increment the iteration count\n",
    "                iter_count += 1\n",
    "\n",
    "                # compute and plot the *validation* loss and accuracy\n",
    "                if (iter_count % plot_every == 0):\n",
    "                    iters.append(iter_count)\n",
    "                    train_loss.append(model.loss(ts))\n",
    "                    train_acc.append(estimate_accuracy(model, train_data))\n",
    "                    val_acc.append(estimate_accuracy(model, validation_data))\n",
    "                    model.cleanup()\n",
    "                    print(f\"Iter {iter_count}. Acc [val:{val_acc[-1]}, train:{train_acc[-1]}] Loss {train_loss[-1]}]\")\n",
    "\t\t\t\tif iter_count >= max_iters:\n",
    "\t\t\t\t\tbreak\n",
    "    finally:\n",
    "        plt.figure()\n",
    "        plt.plot(iters[:len(train_loss)], train_loss)\n",
    "        plt.title(\"Loss over iterations\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(iters[:len(train_acc)], train_acc)\n",
    "        plt.plot(iters[:len(val_acc)], val_acc)\n",
    "        plt.title(\"Accuracy over iterations\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend([\"Train\", \"Validation\"])\n",
    "\n",
    "model= NNModel()\n",
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519374af",
   "metadata": {},
   "source": [
    "## Part 4. Applying the Model\n",
    "\n",
    "In this section, we will use apply the model for sentence completion, and to\n",
    "explore model embeddings.  If you do not have a trained model, you may use\n",
    "the trained weights provided as part of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af716807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_pretrained(NNModel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee1608d",
   "metadata": {},
   "source": [
    "**Task**: The function `make_prediction` has been written for you.\n",
    "It takes as parameters\n",
    "a NNModel model and sentence (a list of words), and produces\n",
    "a prediction for the next word in the sentence.\n",
    "\n",
    "Run the following code to predict what the next word should be in each\n",
    "of the following sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c075e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, sentence):\n",
    "    \"\"\"\n",
    "    Use the model to make a prediction for the next word in the\n",
    "    sentence using the last 3 words (sentence[-3:])\n",
    "    \"\"\"\n",
    "    global vocab_itos\n",
    "    indices = convert_words_to_indices([sentence[-3:]])\n",
    "    X = make_onehot(indices).reshape(-1, 750)\n",
    "    z = model.forward(X)\n",
    "    i = np.argmax(z)\n",
    "    return vocab_itos[i]\n",
    "\n",
    "print(make_prediction(model, ['you', 'are', 'a']))\n",
    "print(make_prediction(model, ['there', 'are', 'no']))\n",
    "print(make_prediction(model, ['yesterday', 'the', 'federal']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157bcd95",
   "metadata": {},
   "source": [
    "Do your predictions make sense? (If all of your predictions are the same,\n",
    "train your model for more iterations, or change the hyper parameters in your\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c15a146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your analysis goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ca36da",
   "metadata": {},
   "source": [
    "While training the `NNModel`, we trained the weight `model.Ww`, which takes a one-hot\n",
    "representation of a word in our vocabulary, and returns a low-dimensional vector\n",
    "representation of that word. \n",
    "These representations, also called **word embeddings** have interesting properties.\n",
    "\n",
    "**Graded Task:** \n",
    "Explain why each *row* of `model.Ww` contains the vector representing\n",
    "of a word. For example `model.Ww[vocab_stoi[\"any\"],:]` contains the\n",
    "vector representation of the word \"any\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f1644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write your explanation here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a706b6c2",
   "metadata": {},
   "source": [
    "One interesting thing about these word embeddings is that distances\n",
    "in these vector representations of words make some sense! To show this,\n",
    "we have provided code below that computes the cosine similarity of\n",
    "every pair of words in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.linalg.norm(model.Ww, axis=1)\n",
    "word_emb_norm = (model.Ww.T / norms).T\n",
    "similarities = np.matmul(word_emb_norm, word_emb_norm.T)\n",
    "\n",
    "# Some example distances. The first one should be larger than the second\n",
    "print(similarities[vocab_stoi['any'], vocab_stoi['many']])\n",
    "print(similarities[vocab_stoi['any'], vocab_stoi['government']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a2bd10",
   "metadata": {},
   "source": [
    "**Task**: Run the below code, which computes the 5 closest words to each of the following words.\n",
    "Replace these words with words of your choice to explore the distances in the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf98535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest(word):\n",
    "    dst = [(w, similarities[vocab_stoi[word], idx])\n",
    "           for w, idx in vocab_stoi.items()] \n",
    "    dst = sorted(dst, key=lambda x: x[1], reverse=True)\n",
    "    return dst[1:6]\n",
    "\n",
    "print(get_closest(\"four\"))\n",
    "print(get_closest(\"go\"))\n",
    "print(get_closest(\"should\"))\n",
    "print(get_closest(\"yesterday\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a56ce",
   "metadata": {},
   "source": [
    "Notice that similar words provided above tend to **occur in similar surrounding words**\n",
    "in a sentence. Why do you think this might be? Consider the architecture used in this model,\n",
    "and what this model is trained to do. (How would replacing a word with another word with a similar\n",
    "embedding change the neural network prediction?)\n",
    "\n",
    "We can also visualize the word embeddings by reducing the dimensionality of\n",
    "the word vectors to 2D. There are many dimensionality reduction techniques\n",
    "that we could use, and we will use an algorithm called t-SNE.\n",
    "(You dont need to know what this is for the lab).\n",
    "Nearby points in this 2-D space are meant to correspond to nearby points\n",
    "in the original, high-dimensional space.\n",
    "\n",
    "The following code runs the t-SNE algorithm and plots the result.\n",
    "Look at the plot and find two clusters of related words.\n",
    "What do the words in each cluster have in common?\n",
    "\n",
    "Note that there is randomness in the initialization of the t-SNE \n",
    "algorithm. If you re-run this code, you may get a different image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdcefaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.manifold\n",
    "tsne = sklearn.manifold.TSNE()\n",
    "Y = tsne.fit_transform(word_emb_norm)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlim(Y[:,0].min(), Y[:, 0].max())\n",
    "plt.ylim(Y[:,1].min(), Y[:, 1].max())\n",
    "for i, w in enumerate(vocab):\n",
    "    plt.text(Y[i, 0], Y[i, 1], w)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0246bedf",
   "metadata": {},
   "source": [
    "```\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
