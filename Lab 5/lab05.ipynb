{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e0e3d5",
   "metadata": {},
   "source": [
    "# CSC413 Lab 5: Transfer Learning and Descent\n",
    "\n",
    "Transfer learning is a technique where we use neural network weights trained\n",
    "to complete one task to complet a different task.\n",
    "In this tutorial, we will go through an example of *transfer learning* to \n",
    "detect American Sign Language (ASL) gestures letters A-I.\n",
    "Although we could train a CNN from scratch,\n",
    "you will see that using CNN weights that are pretrained on a larger dataset and\n",
    "more complex task provides much better results, all with less training.\n",
    "\n",
    "American Sign Language (ASL) is a complete, complex language that employs signs made by \n",
    "moving the hands combined with facial expressions and postures of the body. \n",
    "It is the primary language of many North Americans who are deaf and is one of several \n",
    "communication options used by people who are deaf or hard-of-hearing.\n",
    "\n",
    "The hand gestures representing English alphabets are shown below. This lab focuses on \n",
    "classifying a subset of these hand gesture images using convolutional neural networks.\n",
    "Specifically, given an image of a hand showing one of the letters A-I, we want to detect\n",
    "which letter is being represented.\n",
    "\n",
    "![Image](https://www.lifeprint.com/asl101/fingerspelling/images/abc1280x960.png)\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Analyze the role of batch normalization and other model architecture choice in a neural network.\n",
    "2. Define the double descent phenomenon and explain why it occurs.\n",
    "3. Analyze the shape of the training curve of a convolutional neural network with respect to the double descent phenomenon.\n",
    "4. Apply transfer learning to solve an image classification task.\n",
    "5. Compare transfer learning vs. training a CNN from scratch.\n",
    "6. Identify and suggest corrections for model building issues by inspecting misclassified data.\n",
    "\n",
    "\n",
    "Acknowledgements:\n",
    "\n",
    "- Data is collected from a previous machine learning course APS360. Only data\n",
    "  of students who provided consent is included.\n",
    "\n",
    "Please work in groups of 1-2 during the lab.\n",
    "\n",
    "## Submission\n",
    "\n",
    "If you are working with a partner, start by creating a group on Markus.\n",
    "If you are working alone,\n",
    "click \"Working Alone\".\n",
    "\n",
    "Submit the generated PDF file `lab05.pdf` on Markus \n",
    "**containing all your solutions to the Graded Task**s.\n",
    "Your file must contain your code **and outputs** where applicable,\n",
    "including printed lines and images.\n",
    "Your TA will not run your code for the purpose of grading.\n",
    "\n",
    "For this lab, you should submit the following:\n",
    "\n",
    "- Part 1. Your answer to the question about the splitting of the data into train/validation/test sets. (1 point)\n",
    "- Part 2. Your comparison of the CNN model with and without batch normalization. (1 point)\n",
    "- Part 2. Your comparison of `BatchNorm1d` vs `BatchNorm2d`.  (1 point)\n",
    "- Part 2. Your analysis of the effect of varying the CNN model width. (1 point)\n",
    "- Part 2. Your analysis of the effect of varying weight decay parameter. (1 point)\n",
    "- Part 2. Your analysis of the training curve that illustrates double descent. (1 point)\n",
    "- Part 3. Your implementation of `LinearModel` for transfer learning. (1 point)\n",
    "- Part 3. Your comparison of transfer learning vs the CNN model. (1 point)\n",
    "- Part 4. Your analysis of the confusion matrix. (1 point)\n",
    "- Part 4. Your explanation for how to mitigate an issue we notice by visually inspecting misclassified images. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43e23f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models, torchvision.datasets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b002d",
   "metadata": {},
   "source": [
    "## Part 1. Data\n",
    "\n",
    "We will begin by downloading the data onto Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7735ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download lab data file\n",
    "!wget https://www.cs.toronto.edu/~lczhang/413/asl_data.zip\n",
    "!unzip asl_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30adce42",
   "metadata": {},
   "source": [
    "The file structure we use is intentional,\n",
    "so that we can use `torchvision.datasets.ImageFolder`\n",
    "to help load our data and create labels.\n",
    "\n",
    "You can read what `torchvision.datasets.ImageFolder` does for us here\n",
    "https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b20fb5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"asl_data/train/\" # edit me\n",
    "valid_path = \"asl_data/valid/\" # edit me\n",
    "test_path = \"asl_data/test/\"   # edit me\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(train_path, transform=torchvision.transforms.ToTensor())\n",
    "valid_data = torchvision.datasets.ImageFolder(valid_path, transform=torchvision.transforms.ToTensor())\n",
    "test_data = torchvision.datasets.ImageFolder(test_path, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4733f7",
   "metadata": {},
   "source": [
    "As in previous labs, we can iterate through the dataset one training data point at a time like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ef3b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, t in train_data:\n",
    "    print(x, t)\n",
    "    plt.imshow(x.transpose(2, 0).transpose(0, 1).numpy()) # display an image\n",
    "    break # uncomment if you'd like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82feaded",
   "metadata": {},
   "source": [
    "**Task**: What do the variables `x` and `t` contain? What is the shape of our images?\n",
    "What are our labels? Based on what you learned in Part (a), how were the\n",
    "labels generated from the folder structure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5d3df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your explanation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531aae6c",
   "metadata": {},
   "source": [
    "We saw in the earlier tutorials that PyTorch has a utility to help us\n",
    "creat minibatches with our data. We can use the same DataLoader helper\n",
    "here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870feba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=10, shuffle=True)\n",
    "\n",
    "for x, t in train_loader:\n",
    "    print(x, t)\n",
    "    break # uncomment if you'd like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542b75a1",
   "metadata": {},
   "source": [
    "**Task**: What do the variables `x` and `t` contain? What are their shapes?\n",
    "What data do they contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab1c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your explanation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6999603a",
   "metadata": {},
   "source": [
    "**Task**: How many images are there in the training, validation, and test sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e6f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your explanation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af80151",
   "metadata": {},
   "source": [
    "Notice that there are *fewer* images in the training set, compared to the validation and test sets.\n",
    "This is so that we can explore the effect of having a limited training set.\n",
    "\n",
    "**Graded Task**: The data set is generated by students taking pictures of their hand\n",
    "while making the corresponding gestures. We therefore split the \n",
    "training, validation, and test sets were split so that images generated by\n",
    "a student all belongs in a single data set. In other words, we avoid cases where\n",
    "some students' images are in the training set and others end up in the test set. \n",
    "Why do you think this is important for obtaining a representative test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c511f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your explanation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3539670e",
   "metadata": {},
   "source": [
    "## Part 2. Training a CNN Model\n",
    "\n",
    "For this part, we will be working with this CNN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b6ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, width=4, bn=True):\n",
    "        \"\"\"\n",
    "        A 4-layer convolutional neural network. The first layer has\n",
    "        `width` number of channels, and with each layer we half the\n",
    "        feature width/height and double the number of channels.\n",
    "\n",
    "        If `bn` is set to False, then batch normalization will not run.\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        self.width = width\n",
    "        self.bn = bn\n",
    "        # define all the conv layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,\n",
    "                               out_channels=self.width,\n",
    "                               kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=self.width,\n",
    "                               out_channels=self.width*2,\n",
    "                               kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=self.width*2,\n",
    "                               out_channels=self.width*4,\n",
    "                               kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=self.width*4,\n",
    "                               out_channels=self.width*8,\n",
    "                               kernel_size=3,\n",
    "                               padding=1)\n",
    "        # define all the BN layers\n",
    "        if bn:\n",
    "            self.bn1 = nn.BatchNorm2d(self.width)\n",
    "            self.bn2 = nn.BatchNorm2d(self.width*2)\n",
    "            self.bn3 = nn.BatchNorm2d(self.width*4)\n",
    "            self.bn4 = nn.BatchNorm2d(self.width*8)\n",
    "        # pooling layer has no parameter, so one such layer\n",
    "        # can be shared across all conv layers\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # FC layers\n",
    "        self.fc1 = nn.Linear(self.width * 8 * 14 * 14, 100)\n",
    "        self.fc2 = nn.Linear(100, 9)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        if self.bn:\n",
    "            x = self.bn1(x)\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        if self.bn:\n",
    "            x = self.bn2(x)\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        if self.bn:\n",
    "            x = self.bn3(x)\n",
    "        x = self.pool(torch.relu(self.conv4(x)))\n",
    "        if self.bn:\n",
    "            x = self.bn4(x)\n",
    "        x = x.view(-1, self.width * 8 * 14 * 14)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f64a17",
   "metadata": {},
   "source": [
    "**Task**: The training code is written for you. Train the `CNN()` model for at least 6 epochs, and report\n",
    "on the maximum validation accuracy that you can attain.\n",
    "\n",
    "As your model is training, you might want to move on to the next question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fec04cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data, device=\"cpu\"):\n",
    "    loader = torch.utils.data.DataLoader(data, batch_size=256)\n",
    "    model.to(device)\n",
    "    model.eval() # annotate model for evaluation (important for batch normalization)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for imgs, labels in loader:\n",
    "        labels = labels.to(device)\n",
    "        output = model(imgs.to(device))\n",
    "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += imgs.shape[0]\n",
    "    return correct / total\n",
    "\n",
    "def train_model(model,\n",
    "                train_data,\n",
    "                valid_data,\n",
    "                batch_size=64,\n",
    "                weight_decay=0.0,\n",
    "                learning_rate=0.001,\n",
    "                num_epochs=50,\n",
    "                plot_every=20,\n",
    "                plot=True,\n",
    "                device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")):\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True)\n",
    "    model = model.to(device) # move model to GPU if applicable\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "                           lr=learning_rate,\n",
    "                           weight_decay=weight_decay)\n",
    "    # for plotting\n",
    "    iters, train_loss, train_acc, val_acc = [], [], [], []\n",
    "    iter_count = 0 # count the number of iterations that has passed\n",
    "\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            for imgs, labels in iter(train_loader):\n",
    "                if imgs.size()[0] < batch_size:\n",
    "                    continue\n",
    "                labels = labels.to(device)\n",
    "                imgs = imgs.to(device)\n",
    "                model.train()\n",
    "                out = model(imgs)\n",
    "                loss = criterion(out, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                iter_count += 1\n",
    "                if iter_count % plot_every == 0:\n",
    "                    loss = float(loss)\n",
    "                    tacc = get_accuracy(model, train_data, device)\n",
    "                    vacc = get_accuracy(model, valid_data, device)\n",
    "                    print(\"Iter %d; Loss %f; Train Acc %.3f; Val Acc %.3f\" % (iter_count, loss, tacc, vacc))\n",
    "\n",
    "                    iters.append(iter_count)\n",
    "                    train_loss.append(loss)\n",
    "                    train_acc.append(tacc)\n",
    "                    val_acc.append(vacc)\n",
    "    finally:\n",
    "        plt.figure()\n",
    "        plt.plot(iters[:len(train_loss)], train_loss)\n",
    "        plt.title(\"Loss over iterations\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(iters[:len(train_acc)], train_acc)\n",
    "        plt.plot(iters[:len(val_acc)], val_acc)\n",
    "        plt.title(\"Accuracy over iterations\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend([\"Train\", \"Validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e68f1f",
   "metadata": {},
   "source": [
    "**Task**: Run the training code below. What validation accuracy can be achieved by this CNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(width=4)\n",
    "train_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237f5d8a",
   "metadata": {},
   "source": [
    "## Part 2. Model Architecture, Bias/Variance and Double Descent\n",
    "\n",
    "In this section, we will explore the effect of various aspects of\n",
    "a CNN model architecture. We will pay particluar attention to \n",
    "architecture decisions that affect the bias and variance of the\n",
    "model. Finally, we explore a phenomenon called **double descent**.\n",
    "\n",
    "\n",
    "To begin, let's explore the effect of batch normalization.\n",
    "\n",
    "**Task**: Run the training code below to explore the effect of training *without* batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb007c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(bn=False)\n",
    "train_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a159bd",
   "metadata": {},
   "source": [
    "**Graded Task**: Compare the two sets of training curves above for the CNN model with and without\n",
    "batch normalization. What is the effect of batch normalization on the training loss and accuracy?\n",
    "What about the validation accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec8edbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Include your analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd2c994",
   "metadata": {},
   "source": [
    "**Graded Task**:\n",
    "We used the layer called `BatchNorm2d` in our CNN.\n",
    "What do you think is the difference between `BatchNorm2d` and `BatchNorm1d`?\n",
    "Why are we using `BatchNorm2d` in our CNN? Why would we use `BatchNorm1d` in an MLP?\n",
    "You may wish to consult the PyTorch documentation. (How can you find it?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1614def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833b3715",
   "metadata": {},
   "source": [
    "**Task**: Run the training code below to explore the effect of varying the model width\n",
    "for this particular data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263a0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(width=2, bn=False)\n",
    "train_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(width=4, bn=False)\n",
    "train_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43fdc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(width=16, bn=False)\n",
    "train_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9035402",
   "metadata": {},
   "source": [
    "**Graded Task**: What is the effect of varying the model width above for this particular data set?\n",
    "Do you notice an effect on the training loss? What about the training/validation accuracy?\n",
    "The final validation accuracy?\n",
    "(Your answer may or may not match your expectations. Please answer based on the actual results above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e55214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Include your analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416991f6",
   "metadata": {},
   "source": [
    "**Task**: Run the training code below to explore the effect of weight decay when training a large model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b90c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(width=16, bn=False)\n",
    "train_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f06c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(width=16, bn=True) # try with batch norm on\n",
    "train_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6f0509",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(width=16, bn=True) # try decreasing weight decay parameter\n",
    "train_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=25, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac5cb20",
   "metadata": {},
   "source": [
    "**Graded Task**: What is the effect of setting weight decay to the above value?\n",
    "Do you notice an effect on the training loss? What about the training/validation accuracy?\n",
    "The final validation accuracy?\n",
    "(Again, your answer may or may not match your expectations. Please answer based on the actual results above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c30133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Include your analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2fe01d",
   "metadata": {},
   "source": [
    "**Task**: Note that there is quite a bit of noise in the results that we might obtain above.\n",
    "That is, if you run the same code twice, you may obtain different answers.\n",
    "Why might that be? What are two sources of noise/randomness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e817d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Include your explanation here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53395958",
   "metadata": {},
   "source": [
    "These settings that we have been exporting are hyperparameters that should\n",
    "be tuned when you train a neural network. These hyperparameters interact with\n",
    "one another, and thus we should tune them using the **grid search** strategy\n",
    "mentioned in previous labs.\n",
    "\n",
    "You are not required to perform grid search for this lab, so that we can\n",
    "explore a few other phenomena.\n",
    "\n",
    "One interesting phenomenon is called **double descent**. In statistical learning theory,\n",
    "we expect validation error to *decrease* with increase model capacity, and then *increase*\n",
    "as the model overfits to the number of data points available for training.\n",
    "In practise, in neural networks, we often see that as model capacity increases,\n",
    "validation error first decreases, then increase, and then **decrease again**---hence\n",
    "the name \"double descent\".\n",
    "\n",
    "In fact, the increase in validation error is actually quite subtle.\n",
    "However, what is readily apparent is that in most cases, increasing\n",
    "model capacity does *not* result in a decrease in validation accuracy.\n",
    "\n",
    "**Optional Task**: To illustrate that validation accuracy is unlikely to decrease\n",
    "with increased model parameter, train the below network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0763ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run. \n",
    "# cnn = CNN(width=40, bn=True)\n",
    "# train_model(cnn, train_data, valid_data, batch_size=64, learning_rate=0.001, num_epochs=50, plot_every=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e7e9f5",
   "metadata": {},
   "source": [
    "Double descent is actually not that mysterious. It comes from the fact that \n",
    "when capacity is large enough there are many parameter choices that achieves 100% training accuracy,\n",
    "the neural network optimization procedure is effectively choosing a *best parameters*\n",
    "out of the many that can achieve this perfect training accuracy. This differs from\n",
    "when capacity is low, where the optimization process needs to find a set of parameter choices that\n",
    "best fits the training data---since no choice of parameters fits the training data perfectly.\n",
    "When the capacity is just large enough to be able to find parameters that fit the data,\n",
    "but too small for there be a range of parameter choices available to be able to select a \"best\" one.\n",
    "\n",
    "This twitter thread written by biostatistics professor Daniela Witten\n",
    "also provides an intuitive explanation, using polynomial curve fitting\n",
    "as an example: [https://twitter.com/daniela_witten/status/1292293102103748609](https://twitter.com/daniela_witten/status/1292293102103748609)\n",
    "\n",
    "Double descent explored in depth in this paper here:\n",
    "[https://openreview.net/pdf?id=B1g5sA4twr](https://openreview.net/pdf?id=B1g5sA4twr)\n",
    "This paper highlights that the increase in validation/test error occurs \n",
    "when the training accuracy approximates 100%.\n",
    "Moreover, the double descent phenomena is noticable when varying model capacity (e.g. number of parameters)\n",
    "and when varying the number of iterations/epochs of training.\n",
    "\n",
    "We will attempt to explore the latter effect---i.e. we will train a large model, use a small\n",
    "number of training data points, and explore how each iteration of training impacts validation accuracy.\n",
    "The effect is subtle and, depending on your neural network initialization, you may not see an effect.\n",
    "An examplary training curve is also provided for you to analyze.\n",
    "\n",
    "**Optional Task**: Run the code below to try and reproduce the \"double descent\" phenomena.\n",
    "This code will take a while to run, so you may wish to continue with the remaining questions\n",
    "while it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2340646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a subset of the training data\n",
    "# uncomment to train\n",
    "\n",
    "# train_data_subset, _ =  random_split(train_data, [50, len(train_data)-50])\n",
    "# cnn = CNN(width=20)\n",
    "# train_model(cnn,\n",
    "#             train_data_subset,\n",
    "#             valid_data,\n",
    "#             batch_size=50, # set batch_size=len(train_data_subset) to minimize training noise\n",
    "#             num_epochs=200,\n",
    "#             plot_every=1,  # plot every epoch (this is slow)\n",
    "#             learning_rate=0.0001)  # choose a low learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030613b7",
   "metadata": {},
   "source": [
    "For reference, here is the our training curve showing the loss and accuracy over 200 iterations:\n",
    "\n",
    "<img src=\"https://www.cs.toronto.edu/~lczhang/413/double_descent_loss.png\" width=400>\n",
    "<img src=\"https://www.cs.toronto.edu/~lczhang/413/double_descent.png\" width=400>\n",
    "\n",
    "It might not be possible to consistently reproduce this result (e.g., due to initialization),\n",
    "so it is totally reasonable for your figure to look different!\n",
    "\n",
    "\n",
    "**Task**: In the provided training curve,\n",
    "during which iterations does the validation accuracy initially increase\n",
    "(i.e. validation error decrease)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963f8c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Include your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd905af",
   "metadata": {},
   "source": [
    "**Graded Task**: In the provided training curve,\n",
    "during which iterations do the validation accuracy decrease slightly?\n",
    "Approximately what training accuracy is achieved at this piont?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7bc80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Include your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c339bd",
   "metadata": {},
   "source": [
    "**Task**: In the provided training curve,\n",
    "during which iterations do the validation accuracy increase for a second time\n",
    "(i.e. validation error descends for a second time)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d4c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Include your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbff4a2",
   "metadata": {},
   "source": [
    "## Part 3. Transfer Learning\n",
    "\n",
    "For many image classification tasks, it is generally not a good idea to train a\n",
    "very large deep neural network model from scratch due to the enormous compute\n",
    "requirements and lack of sufficient amounts of training data.\n",
    "\n",
    "A better option is to try using an existing model that performs a\n",
    "similar task to the one you need to solve. This method of utilizing a\n",
    "pre-trained network for other similar tasks is broadly termed\n",
    "**Transfer Learning**. In this assignment, we will use Transfer Learning\n",
    "to extract features from the hand gesture images. Then, train a smaller\n",
    "network to use these features as input and classify the hand gestures.\n",
    "\n",
    "As you have learned from the CNN lecture, convolution layers extract various\n",
    "features from the images which get utilized by the fully connected layers\n",
    "for correct classification. AlexNet architecture played a pivotal role in\n",
    "establishing Deep Neural Nets as a go-to tool for image classification\n",
    "problems and we will use an ImageNet pre-trained AlexNet model to\n",
    "extract features in this assignment.\n",
    "\n",
    "Here is the code to load the AlexNet network, with pretrained weights.\n",
    "When you first run the code, PyTorch will download the pretrained weights\n",
    "from the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb3a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "alexnet = torchvision.models.alexnet(pretrained=True)\n",
    "\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8779999d",
   "metadata": {},
   "source": [
    "As you can see, the `alexnet` model is split up into two components:\n",
    "`alexnet.features` and \n",
    "`alexnet.classifier`.  The first neural network component, `alexnet.features`,\n",
    "is used to\n",
    "computed convolutional features, which is taken as input in `alexnet.classifier`.\n",
    "\n",
    "The neural network `alexnet.features` expects an image tensor of shape\n",
    "Nx3x224x224 as inputs and it will output a tensor of shape Nx256x6x6 . (N = batch size).\n",
    "\n",
    "Here is an example code snippet showing how you can compute the AlexNet\n",
    "features for some images (your actual code might be different):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bcc4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_data[0]\n",
    "features = alexnet.features(img.unsqueeze(0)).detach()\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a241555",
   "metadata": {},
   "source": [
    "Note that the `.detach()` at the end will be necessary in your code. The reason is that\n",
    "PyTorch automatically builds computation graphs to be able to backpropagate\n",
    "graidents. If we did not explicitly \"detach\" this tensor from the AlexNet portion\n",
    "of the computation graph, PyTorch might try to backpropagate gradients to the AlexNet\n",
    "weight and tune the AlexNet weights.\n",
    "\n",
    "**Task**: Compute the AlexNet features for each of your training, validation, and test data\n",
    "by completing the function `compute_features`.\n",
    "The code below creates three new arrays called `train_data_fets`, `valid_data_fets`\n",
    "and `test_data_fets`. Each of these arrays contains tuples of the form \n",
    "`(alexnet_features, label)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674de7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(data):\n",
    "    fets = []\n",
    "    for img, t in data:\n",
    "        features = None  # TODO\n",
    "        fets.append((features, t),)\n",
    "    return fets\n",
    "\n",
    "train_data_fets = compute_features(train_data)\n",
    "valid_data_fets = compute_features(valid_data)\n",
    "test_data_fets = compute_features(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdda13c",
   "metadata": {},
   "source": [
    "In the rest of this part of the lab, we will test two models that\n",
    "will take **as input** these AlexNet features, and make a prediction\n",
    "for which letter the hand gesture represents.\n",
    "The two models are\n",
    "a linear model, a two-layer MLP.\n",
    "We will compare the performance of these two models.\n",
    "\n",
    "**Graded Task**: Complete the definition of the `LinearModel` class,\n",
    "which is a linear model (e.g., logistic regression).\n",
    "This model should as input these AlexNet features, and make a prediction\n",
    "for which letter the hand gesture represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86dd85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        # TODO: What layer need to be initialized?\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 256 * 6 * 6) # flatten the input\n",
    "        z = None # TODO: What computation needs to be performed?\n",
    "        return z\n",
    "\n",
    "m_linear = LinearModel()\n",
    "m_linear(train_data_fets[0][0]) # this should produce a(n unnormalized) prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6064a9",
   "metadata": {},
   "source": [
    "**Task**:\n",
    "Train a `LinearModel()` for at least 6 epochs, and report\n",
    "on the maximum validation accuracy that you can attain.\n",
    "We should still be able to use the `train_model` function, but\n",
    "make sure to provide the AlexNet features as input (and not the\n",
    "image features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba203ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_linear = LinearModel()\n",
    "# TODO: Train the linear model. Include your output in your submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3422f2ee",
   "metadata": {},
   "source": [
    "**Graded Task**: Compare this model with the CNN() models that we trained\n",
    "earlier. How does this model perform in terms of validation accuracy?\n",
    "What about in terms of the time it took to train this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ddec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your observation goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d1fa40",
   "metadata": {},
   "source": [
    "**Task**:\n",
    "We decide to use AlexNet features as input to our MLP, and avoided tuning AlexNet\n",
    "weights. However, we could have considered AlexNet to be a part of our model, and\n",
    "continue to tune AlexNet weights to improve our model performance. What are the\n",
    "advantages and disadvantages of continuing to tune AlexNet weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd943d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420aaf0c",
   "metadata": {},
   "source": [
    "## Part 4. Data\n",
    "\n",
    "**Task**: Report the test accuracy on this transfer learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b80b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1781714",
   "metadata": {},
   "source": [
    "**Task**: Use this code below to construct the confusion matrix for this model\n",
    "over the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126a7048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import sklearn\n",
    "label = \"ABCDEFGHI\"\n",
    "def plot_confusion(model, data):\n",
    "    n = 0\n",
    "    ts = []\n",
    "    ys = []\n",
    "    for x, t in data:\n",
    "        z = model(x.unsqueeze(0))\n",
    "        y = int(torch.argmax(z))\n",
    "        ts.append(t)\n",
    "        ys.append(y)\n",
    "\n",
    "    cm = confusion_matrix(ts, ys)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label)\n",
    "    plt.figure()\n",
    "    disp.plot()\n",
    "\n",
    "plot_confusion(m_linear, test_data_fets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a8c9e9",
   "metadata": {},
   "source": [
    "**Graded Task**: Which class is most likely mistaken as another?\n",
    "Is this reasonable? (i.e. is that class particularly challenging, or \n",
    "very similar to another class?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Include your analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536ded41",
   "metadata": {},
   "source": [
    "**Task**: In order to understand where errors come from, it is *crucial* that\n",
    "we explore why and how our models fail. A first step is to visually inspect the\n",
    "test data points where failure occurs. That way, we can identify what we can do \n",
    "to prevent/fix errors before our models are deployed.\n",
    "\n",
    "Run the below code to display images in the test set that our model *misclassifies*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc28e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x, t) in enumerate(test_data_fets):\n",
    "    y = int(torch.argmax(m_linear(x)))\n",
    "    if not (y == t):\n",
    "        plt.figure()\n",
    "        plt.imshow(test_data[i][0].transpose(0,1).transpose(1,2).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742b133e",
   "metadata": {},
   "source": [
    "**Task**: By visually inspecting these misclassified images, we see that there are\n",
    "two main reasons for misclassification. What reason for misclassification is\n",
    "due to a mistake in the formatting of the test set images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e14d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b1126",
   "metadata": {},
   "source": [
    "**Graded Task**: We also see a much more serious issue, where gestures made by\n",
    "individuals with darker skin tones may be more frequently misclasified.\n",
    "This result suggests that errors in the model may impact some groups more than\n",
    "others. What steps should we take to mitigate this issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa32952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
